# -*- coding: utf-8 -*-
"""SPS_Corps.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CEOSFgcKl8gEwmpcSa9J9KR9o3g_qpA4
"""

# ==========================================
# CELL 1: SETUP & INSTALLATIONS
# ==========================================
print("ðŸš€ Installing required packages...")
!pip install prophet -q
!pip install plotly -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import json
import warnings
warnings.filterwarnings('ignore')

# Deep Learning Imports
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from prophet import Prophet
from google.colab import files

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

print(f"âœ… Setup Complete. TensorFlow Version: {tf.__version__}")

# ==========================================
# CELL 2: LOAD DATA & FIX COLUMNS
# ==========================================
print("ðŸ“‚ Please upload 'train.csv' from your computer:")
uploaded = files.upload()

if 'train.csv' not in uploaded:
    raise ValueError("âŒ Error: Please upload a file named 'train.csv'")

# Load data
df = pd.read_csv('train.csv')

# --- ðŸš¨ AUTO-FIX FOR YOUR DATA ðŸš¨ ---
# 1. Rename 'IsHoliday' to 'Holiday_Flag'
if 'IsHoliday' in df.columns:
    print("ðŸ”§ Fixing Column Name: 'IsHoliday' -> 'Holiday_Flag'")
    df.rename(columns={'IsHoliday': 'Holiday_Flag'}, inplace=True)

# 2. Convert True/False to 1/0
if df['Holiday_Flag'].dtype == bool or df['Holiday_Flag'].dtype == object:
    print("ðŸ”§ Fixing Values: Converting True/False -> 1/0")
    df['Holiday_Flag'] = df['Holiday_Flag'].astype(int)
# ------------------------------------

# Convert Date to datetime object
df['Date'] = pd.to_datetime(df['Date'])

print("\nâœ… Data Loaded & Fixed Successfully!")
print(df.info())

# ==========================================
# CELL 3: PREPROCESSING & SPLITTING
# ==========================================
print("âš™ï¸ Aggregating sales by week...")

# Aggregate to get Total Weekly Sales across all stores
weekly_sales = df.groupby('Date').agg({
    'Weekly_Sales': 'sum',
    'Holiday_Flag': 'max'  # If any store had a holiday, the week is a holiday
}).reset_index()

# Sort by date
weekly_sales = weekly_sales.sort_values('Date').reset_index(drop=True)

# Split 80% Train, 20% Test
split_idx = int(len(weekly_sales) * 0.8)
train_data = weekly_sales[:split_idx].copy()
test_data = weekly_sales[split_idx:].copy()

print(f"âœ… Data Split Complete:")
print(f"   Training Weeks: {len(train_data)}")
print(f"   Testing Weeks:  {len(test_data)}")

# ==========================================
# CELL 4: EXPLORATORY DATA ANALYSIS (EDA)
# ==========================================
plt.figure(figsize=(15, 6))
plt.plot(train_data['Date'], train_data['Weekly_Sales'], label='Training Data')
plt.plot(test_data['Date'], test_data['Weekly_Sales'], label='Test Data', color='orange')
plt.title('Walmart Total Weekly Sales History', fontsize=16)
plt.ylabel('Sales ($)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Holiday Check
holidays = weekly_sales[weekly_sales['Holiday_Flag'] == 1]
print(f"ðŸŽ‰ Identified {len(holidays)} Holiday Weeks in the dataset.")

# ==========================================
# CELL 5: TRAIN PROPHET MODEL
# ==========================================
print("ðŸ”® Training Prophet Model...")

# 1. Format data for Prophet (needs 'ds' and 'y' columns)
prophet_train = train_data[['Date', 'Weekly_Sales']].rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})

# 2. Identify Holidays
holidays_df = train_data[train_data['Holiday_Flag'] == 1][['Date']].copy()
holidays_df.rename(columns={'Date': 'ds'}, inplace=True)
holidays_df['holiday'] = 'walmart_holiday'

# 3. Train Model
model_prophet = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=False,
    daily_seasonality=False,
    holidays=holidays_df,
    interval_width=0.95
)
model_prophet.fit(prophet_train)

# 4. Forecast 12 Weeks Future
future = model_prophet.make_future_dataframe(periods=len(test_data) + 12, freq='W')
forecast_prophet = model_prophet.predict(future)

print("âœ… Prophet Training Complete.")

# ==========================================
# CELL 6: TRAIN LSTM MODEL
# ==========================================
print("ðŸ§  Training LSTM Neural Network...")

# 1. Scale Data (0 to 1)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_train = scaler.fit_transform(train_data[['Weekly_Sales']])
scaled_test = scaler.transform(test_data[['Weekly_Sales']])

# 2. Create Sequences (Use past 12 weeks to predict next week)
def create_sequences(dataset, look_back=12):
    X, Y = [], []
    for i in range(len(dataset) - look_back):
        X.append(dataset[i:(i + look_back), 0])
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

LOOK_BACK = 12
X_train, y_train = create_sequences(scaled_train, LOOK_BACK)
X_test, y_test = create_sequences(scaled_test, LOOK_BACK)

# Reshape for LSTM [samples, time steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# 3. Build & Train Model
model_lstm = Sequential()
model_lstm.add(LSTM(100, return_sequences=True, input_shape=(LOOK_BACK, 1)))
model_lstm.add(Dropout(0.2))
model_lstm.add(LSTM(50))
model_lstm.add(Dropout(0.2))
model_lstm.add(Dense(1))

model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')

history = model_lstm.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.1,
    verbose=1
)

print("âœ… LSTM Training Complete.")

# ==========================================
# CELL 7: GENERATE HYBRID FORECAST
# ==========================================
print("ðŸ¤ Generating Hybrid Forecast (Prophet + LSTM)...")

# 1. Generate LSTM Future Predictions (Recursive)
# Start with last 12 weeks of data
curr_sequence = scaled_test[-LOOK_BACK:].copy()
lstm_future = []

for i in range(12): # Predict 12 weeks ahead
    # Predict next step
    next_pred = model_lstm.predict(curr_sequence.reshape(1, LOOK_BACK, 1), verbose=0)
    lstm_future.append(next_pred[0, 0])
    # Update sequence (drop oldest, add newest)
    curr_sequence = np.append(curr_sequence[1:], next_pred[0, 0]).reshape(-1, 1)

# Inverse scale to get real dollars
lstm_future_dollars = scaler.inverse_transform(np.array(lstm_future).reshape(-1, 1))

# 2. Get Prophet Future Predictions
prophet_future = forecast_prophet.tail(12)['yhat'].values.reshape(-1, 1)

# 3. Combine (70% Prophet, 30% LSTM)
hybrid_forecast = (0.7 * prophet_future) + (0.3 * lstm_future_dollars)

# Create Final DataFrame
future_dates = pd.date_range(start=test_data['Date'].max() + timedelta(days=7), periods=12, freq='W')
final_df = pd.DataFrame({
    'Date': future_dates,
    'Prophet': prophet_future.flatten(),
    'LSTM': lstm_future_dollars.flatten(),
    'Hybrid': hybrid_forecast.flatten()
})

print("âœ… Hybrid Forecast Generated!")
print(final_df.head())

# ==========================================
# CELL 8: DETECT ANOMALIES
# ==========================================
print("ðŸš¨ Detecting Anomalies...")

# Method: Mean +/- 2 Standard Deviations
mean_sales = weekly_sales['Weekly_Sales'].mean()
std_sales = weekly_sales['Weekly_Sales'].std()
threshold = 2.0

weekly_sales['Z_Score'] = (weekly_sales['Weekly_Sales'] - mean_sales) / std_sales
weekly_sales['Anomaly'] = weekly_sales['Z_Score'].abs() > threshold

anomalies_df = weekly_sales[weekly_sales['Anomaly'] == True].copy()

print(f"âœ… Found {len(anomalies_df)} Anomalies.")

# ==========================================
# CELL 9: EXPORT & DOWNLOAD FILES
# ==========================================
print("ðŸ’¾ Saving files for Streamlit...")

# 1. Save Forecast CSV
final_df.to_csv("forecast_results.csv", index=False)

# 2. Save Anomalies CSV
anomalies_df[['Date', 'Weekly_Sales', 'Anomaly']].to_csv("anomalies.csv", index=False)

# 3. Generate & Save Text Summary (MD)
avg_pred = final_df['Hybrid'].mean()
anom_count = len(anomalies_df)

summary_text = f"""
### ðŸ“Š Executive Sales Report
**Generated on:** {datetime.now().strftime('%Y-%m-%d')}

#### 1. Forecast Overview (Next 12 Weeks)
* **Projected Average Sales:** ${avg_pred:,.2f}
* **Trend:** The hybrid model combines seasonal trends (Prophet) with recent patterns (LSTM).
* **Key Drivers:** Seasonality and recent holiday impacts were key factors in this prediction.

#### 2. Historical Anomalies
* **Total Detected:** {anom_count} significant events.
* **Method:** Z-Score Analysis (> 2.0 Standard Deviations).
* **Context:** These spikes typically correlate with major events like Super Bowl, Thanksgiving, and Christmas.

#### 3. Model Technicals
* **Architecture:** Hybrid Ensemble (70% Prophet + 30% LSTM).
* **Deep Learning:** LSTM trained for 50 epochs with a 12-week lookback window.
"""

with open("summary.md", "w") as f:
    f.write(summary_text)

# 4. Download Files
print("ðŸ“¥ Downloading files... (Please allow pop-ups if blocked)")
files.download('forecast_results.csv')
files.download('anomalies.csv')
files.download('summary.md')

print("ðŸŽ‰ ALL DONE! Upload these 3 files to your Streamlit App.")
